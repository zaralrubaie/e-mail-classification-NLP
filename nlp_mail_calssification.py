# -*- coding: utf-8 -*-
"""NLP-mail classification.ipynb

Automatically generated by Colab.
Original file is located at:
    https://colab.research.google.com/drive/1AnrFUkYDVlqrxexveFLsCPVryL4C6H67
"""

# ==============================
# 1. Import required libraries
# ==============================
import pandas as pd
import re
import spacy

# ==============================
# 2. Load train and test datasets
# ==============================
df_test = pd.read_csv('/kaggle/input/email-classification-nlp/SMS_test.csv', encoding='latin-1')
df_train = pd.read_csv('/kaggle/input/email-classification-nlp/SMS_train.csv', encoding='latin-1')

# Quick peek at first 10 rows of training data
df_train.head(10)

# ==============================
# 3. Drop unnecessary columns
# ==============================
df_test = df_test.drop(['S. No.'], axis=1)
df_train = df_train.drop(['S. No.'], axis=1)

# ==============================
# 4. Detect weird non-ASCII characters
# ==============================
def find_non_ascii(text):
    return ''.join([char for char in text if ord(char) > 127])

df_train["weird_chars"] = df_train["Message_body"].apply(find_non_ascii)
df_train[df_train["weird_chars"] != ""]

# ==============================
# 5. Clean text: remove non-ASCII + punctuation
# ==============================
def clean_ascii_extended(text):
    text = text.encode("ascii", "ignore").decode("ascii")  # remove non-ASCII
    text = text.lower()                                   # lowercase
    text = re.sub(r'[^a-z\s]', '', text)                  # keep only letters
    text = re.sub(r'\s+', ' ', text).strip()              # remove extra spaces
    return text

df_train["Cleaned_Message"] = df_train["Message_body"].apply(clean_ascii_extended)
df_test["Cleaned_Message"] = df_test["Message_body"].apply(clean_ascii_extended)

# ==============================
# 6. Install contractions library (for text normalization)
# ==============================
!pip install contractions

# ==============================
# 7. Setup SpaCy + stopwords
# ==============================
from spacy.lang.en.stop_words import STOP_WORDS
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])

# Slang dictionary for normalization
slang_dict = {
    "u": "you", "ur": "your", "lol": "laughing",
    "btw": "by the way", "idk": "I do not know",
    "imo": "in my opinion", "wanna": "want to",
    "gonna": "going to", "fr": "for", "b4": "before",
    "lei": "", "lor": ""
    # extend dictionary if needed
}

# ==============================
# 8. Custom cleaning pipeline
# ==============================
def clean_text(text):
    text = text.encode("ascii", "ignore").decode("ascii")          # remove non-ASCII
    text = text.lower()                                            # lowercase
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)            # remove URLs
    text = re.sub(r'[^a-z\s]', ' ', text)                          # remove punctuation
    text = re.sub(r'\s+', ' ', text).strip()                       # normalize spaces
    # replace slang words
    words = text.split()
    words = [slang_dict.get(word, word) for word in words]
    text = " ".join(words)
    # remove stopwords with SpaCy
    doc = nlp(text)
    tokens = [token.text for token in doc if token.text not in STOP_WORDS and len(token.text) > 2]
    return " ".join(tokens)

# Apply cleaning to datasets
df_train['Cleaned_Message'] = df_train['Message_body'].apply(clean_text)
df_test['Cleaned_Message'] = df_test['Message_body'].apply(clean_text)

# ==============================
# 9. Lemmatization with SpaCy
# ==============================
def spacy_lemmatize(text):
    doc = nlp(text)
    return " ".join([
        token.lemma_ for token in doc
        if token.lemma_ not in STOP_WORDS and token.is_alpha
    ])

df_train['Final_Text'] = df_train['Cleaned_Message'].apply(spacy_lemmatize)
df_test['Final_Text'] = df_test['Cleaned_Message'].apply(spacy_lemmatize)

# ==============================
# 10. Feature extraction (TF-IDF)
# ==============================
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = df_train['Final_Text'].tolist()

tfidf = TfidfVectorizer(
    max_features=5000,        # limit vocab size
    ngram_range=(1, 2),       # unigrams + bigrams
    stop_words='english',
    min_df=5,                 # ignore rare terms
    max_df=0.7                # ignore too frequent terms
)

X_train_tfidf = tfidf.fit_transform(corpus)
X_test_tfidf = tfidf.transform(df_test['Final_Text'].tolist())

# ==============================
# 11. Encode labels
# ==============================
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df_train['Label'] = le.fit_transform(df_train['Label'])
df_test['Label'] = le.transform(df_test['Label'])

# Drop unused columns
df_test = df_test.drop(['Message_body', 'Cleaned_Message'], axis=1)
df_train = df_train.drop(['Message_body', 'weird_chars', 'Cleaned_Message'], axis=1)

# ==============================
# 12. Train baseline Logistic Regression model
# ==============================
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

model = LogisticRegression(max_iter=1000)
model.fit(X_train_tfidf, df_train['Label'])

# Predictions on test set
y_pred = model.predict(X_test_tfidf)

# Evaluation
print(classification_report(df_test['Label'], y_pred))
print("Accuracy:", accuracy_score(df_test['Label'], y_pred))

# ==============================
# 13. Cross-validation with StratifiedKFold
# ==============================
from sklearn.model_selection import StratifiedKFold, cross_val_score
model = LogisticRegression(max_iter=1000)

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
X = tfidf.transform(df_train['Final_Text'])
y = df_train['Label']

scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')
print("Cross-validation accuracy scores:", scores)
print("Mean accuracy:", scores.mean())

# ==============================
# 14. Train tuned Logistic Regression (C=100)
# ==============================
best_model = LogisticRegression(C=100, max_iter=1000)
best_model.fit(X_train_tfidf, df_train['Label'])

X_test = tfidf.transform(df_test['Final_Text'])
y_test = df_test['Label']

test_accuracy = best_model.score(X_test, y_test)
print("Test accuracy:", test_accuracy)

# ==============================
# 15. Save predictions for analysis
# ==============================
prediction_df = pd.DataFrame({
    'Final_Text': df_test['Final_Text'],
    'Actual_Label': df_test['Label'],
    'Predicted_Label': y_pred,
    'Correct': df_test['Label'] == y_pred
})

print(prediction_df.head(10))  # preview first predictions
prediction_df.to_csv('predictions_logistic_regression.csv', index=False)
